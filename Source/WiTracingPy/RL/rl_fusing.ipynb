{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem formulation:\n",
    "\n",
    "Formulate the problem as an MDP (Markov Decision Process). The state space can represent the current estimated positions, orientations, and velocities of all pedestrians, as well as the camera detections. Actions could correspond to different ways of matching and fusing the IMU and visual data for each pedestrian. The reward function should encourage accurate matching and localization.\n",
    "\n",
    "When dealing with multiple people, each with their own IMU and visual detections, using RL for sensor fusion and localization can be even more beneficial. In this scenario, the reinforcement learning strategy can help in the following ways:\n",
    "\n",
    "Multi-agent coordination: With multiple IMUs and visual detections, the RL strategy can be extended to a multi-agent setting. In this case, each agent (person) can have its own RL agent to fuse their respective IMU and visual data. Additionally, these RL agents can cooperate with each other to improve localization accuracy, for example by sharing information about their relative positions or collaborating to reduce uncertainties.\n",
    "\n",
    "Handling occlusions and data association: In situations where people may be occluding each other in the camera's field of view, or where visual detections might be ambiguous, RL can help to resolve these issues by learning the best strategy to associate the correct visual detection with the corresponding IMU data. This can be particularly important when dealing with a crowded environment.\n",
    "\n",
    "Adaptive sensor fusion: In a multi-person setting, the quality of sensor data can vary among individuals due to differences in sensor placement, calibration, or other factors. An RL-based approach can adapt the sensor fusion strategy for each person individually, optimizing the performance for each user and ensuring better localization accuracy overall.\n",
    "\n",
    "Anomaly detection and outlier handling: The RL agent can learn to detect and handle anomalies in the sensor data or visual detections, such as faulty IMU readings or false-positive detections. This ability to recognize and deal with such issues can lead to a more robust and reliable localization system for multiple people.\n",
    "\n",
    "Dynamic group formation: The RL agents can learn to form dynamic groups based on their proximity, relevance, or other factors, and adapt their sensor fusion strategies accordingly. This can help to improve localization accuracy, especially in complex environments where people are moving and interacting with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "os.chdir('..')\n",
    "import pandas as pd\n",
    "wifi_df = pd.read_csv('../Services/data/raw.csv')\n",
    "cam_df = pd.read_csv('../Services/data/cam_raw.csv')\n",
    "imu_df = pd.read_csv('../Services/data/imu_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def draw_grid_map(data, vmax=255):\n",
    "\n",
    "    # Create a colormap from dark blue to light blue\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\"\", [\"white\", \"black\"])\n",
    "\n",
    "    # Create a grid color map\n",
    "    plt.imshow(data, cmap=cmap, origin=\"lower\", vmin=0, vmax=vmax)\n",
    "\n",
    "    # Add a colorbar to show the intensity of the colors\n",
    "    # plt.colorbar()\n",
    "\n",
    "    # Set the ticks to show the index of the 2D array\n",
    "    plt.xticks(np.arange(-0.5, data.shape[1], 1), [])\n",
    "    plt.yticks(np.arange(-0.5, data.shape[0], 1), [])\n",
    "\n",
    "    # Customize the grid appearance\n",
    "    plt.gca().set_xticklabels([])\n",
    "    plt.gca().set_yticklabels([])\n",
    "    plt.gca().xaxis.set_ticks_position('none')\n",
    "    plt.gca().yaxis.set_ticks_position('none')\n",
    "\n",
    "    # Show the grid lines\n",
    "    plt.grid(True, which='both', color='k', linewidth=1)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL agent is installed on each pedestrian's phone and the state space of the IMU contains only the local estimated position, orientation, and velocity, the approach to fuse data and match pedestrians can be adapted as follows:\n",
    "\n",
    "Data association: Each RL agent will need to consider all the pedestrian detections from the camera in order to associate its local IMU data with the correct visual detection. The agent can learn a policy that takes into account its position, orientation, and velocity estimates from the IMU data, as well as the global visual detections, to find the best match.\n",
    "\n",
    "Feature extraction and comparison: To facilitate data association, the RL agent may need to extract relevant features from both the IMU and visual data that can help in matching pedestrians. For example, it could use the IMU-derived velocity and orientation information to predict its appearance or location in the camera's field of view. The agent can then compare these predictions with the actual visual detections to find the best match.\n",
    "\n",
    "Collaboration among pedestrians: If each pedestrian is equipped with an RL agent, they can cooperate by sharing information about their positions, orientations, and velocities. This collaboration can help improve the data association process by providing additional context and reducing ambiguities. The agents can communicate and collaborate in a decentralized manner, updating their policies and estimates based on the shared information.\n",
    "\n",
    "Temporal consistency: The RL agent can still exploit temporal consistency to improve the matching process. By considering the historical data, the agent can learn a more robust policy that can handle temporary occlusions or noisy measurements. This can be achieved by incorporating memory components in the RL architecture, such as LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) layers.\n",
    "\n",
    "By adjusting the approach to handle global visual detections, the RL agents on each pedestrian's phone can still effectively fuse local IMU data with the camera's pedestrian detections and match the pedestrians correctly. This can result in more accurate and robust localization in dynamic environments with multiple pedestrians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "               IMU_ID  acceleration_x  acceleration_y  acceleration_z  \\\n0     BP_TargetAI_C_0    0.000000e+00    0.000000e+00             0.0   \n9     BP_TargetAI_C_0    0.000000e+00    0.000000e+00             0.0   \n18    BP_TargetAI_C_0    7.498383e+02   -1.556524e+01             0.0   \n27    BP_TargetAI_C_0    7.498384e+02   -1.556524e+01             0.0   \n36    BP_TargetAI_C_0    1.468471e+03   -3.048270e+01             0.0   \n...               ...             ...             ...             ...   \n6572  BP_TargetAI_C_0    1.037091e-10    9.362627e-12             0.0   \n6579  BP_TargetAI_C_0   -9.084924e-11   -3.694285e-11             0.0   \n6586  BP_TargetAI_C_0    3.482608e-11    4.399083e-11             0.0   \n6593  BP_TargetAI_C_0    2.978892e-12   -3.723615e-11             0.0   \n6600  BP_TargetAI_C_0   -6.848660e-11   -4.565773e-12             0.0   \n\n      orientation_x  orientation_y  orientation_z      timestamp  \n0                 0              0       0.000000  1682525442561  \n9                 0              0       0.000000  1682525442633  \n18                0              0      22.975561  1682525442686  \n27                0              0       0.000000  1682525442735  \n36                0              0       0.000000  1682525442778  \n...             ...            ...            ...            ...  \n6572              0              0       0.000000  1682525777652  \n6579              0              0       0.000000  1682525777691  \n6586              0              0       0.000000  1682525777728  \n6593              0              0       0.000000  1682525777767  \n6600              0              0       0.000000  1682525777804  \n\n[851 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IMU_ID</th>\n      <th>acceleration_x</th>\n      <th>acceleration_y</th>\n      <th>acceleration_z</th>\n      <th>orientation_x</th>\n      <th>orientation_y</th>\n      <th>orientation_z</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1682525442561</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1682525442633</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>7.498383e+02</td>\n      <td>-1.556524e+01</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>22.975561</td>\n      <td>1682525442686</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>7.498384e+02</td>\n      <td>-1.556524e+01</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1682525442735</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>1.468471e+03</td>\n      <td>-3.048270e+01</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1682525442778</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6572</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>1.037091e-10</td>\n      <td>9.362627e-12</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1682525777652</td>\n    </tr>\n    <tr>\n      <th>6579</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>-9.084924e-11</td>\n      <td>-3.694285e-11</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1682525777691</td>\n    </tr>\n    <tr>\n      <th>6586</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>3.482608e-11</td>\n      <td>4.399083e-11</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1682525777728</td>\n    </tr>\n    <tr>\n      <th>6593</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>2.978892e-12</td>\n      <td>-3.723615e-11</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1682525777767</td>\n    </tr>\n    <tr>\n      <th>6600</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>-6.848660e-11</td>\n      <td>-4.565773e-12</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1682525777804</td>\n    </tr>\n  </tbody>\n</table>\n<p>851 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imu_df[imu_df['IMU_ID'] == 'BP_TargetAI_C_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "               Ped_ID         x         y      world_x     world_y    los  \\\n0     BP_TargetAI_C_0  0.000000  0.000000 -2083.305176    2.227457  False   \n9     BP_TargetAI_C_0  0.000000  0.000000 -2083.305176    2.227457  False   \n18    BP_TargetAI_C_0  0.000000  0.000000 -2081.296631    2.185759  False   \n27    BP_TargetAI_C_0  0.000000  0.000000 -2077.225830    2.101257  False   \n36    BP_TargetAI_C_0  0.000000  0.000000 -2069.872314    1.948612  False   \n...               ...       ...       ...          ...         ...    ...   \n6578  BP_TargetAI_C_0  0.559425  0.471233 -1677.469727 -583.596985   True   \n6585  BP_TargetAI_C_0  0.559382  0.468700 -1674.963013 -588.665588   True   \n6592  BP_TargetAI_C_0  0.559339  0.466114 -1672.385132 -593.877747   True   \n6599  BP_TargetAI_C_0  0.559297  0.463586 -1669.847412 -599.009033   True   \n6606  BP_TargetAI_C_0  0.559256  0.461129 -1667.363647 -604.030945   True   \n\n          timestamp  \n0     1682525442561  \n9     1682525442633  \n18    1682525442686  \n27    1682525442735  \n36    1682525442777  \n...             ...  \n6578  1682525777652  \n6585  1682525777690  \n6592  1682525777728  \n6599  1682525777767  \n6606  1682525777804  \n\n[852 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ped_ID</th>\n      <th>x</th>\n      <th>y</th>\n      <th>world_x</th>\n      <th>world_y</th>\n      <th>los</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2083.305176</td>\n      <td>2.227457</td>\n      <td>False</td>\n      <td>1682525442561</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2083.305176</td>\n      <td>2.227457</td>\n      <td>False</td>\n      <td>1682525442633</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2081.296631</td>\n      <td>2.185759</td>\n      <td>False</td>\n      <td>1682525442686</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2077.225830</td>\n      <td>2.101257</td>\n      <td>False</td>\n      <td>1682525442735</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2069.872314</td>\n      <td>1.948612</td>\n      <td>False</td>\n      <td>1682525442777</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6578</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.559425</td>\n      <td>0.471233</td>\n      <td>-1677.469727</td>\n      <td>-583.596985</td>\n      <td>True</td>\n      <td>1682525777652</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.559382</td>\n      <td>0.468700</td>\n      <td>-1674.963013</td>\n      <td>-588.665588</td>\n      <td>True</td>\n      <td>1682525777690</td>\n    </tr>\n    <tr>\n      <th>6592</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.559339</td>\n      <td>0.466114</td>\n      <td>-1672.385132</td>\n      <td>-593.877747</td>\n      <td>True</td>\n      <td>1682525777728</td>\n    </tr>\n    <tr>\n      <th>6599</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.559297</td>\n      <td>0.463586</td>\n      <td>-1669.847412</td>\n      <td>-599.009033</td>\n      <td>True</td>\n      <td>1682525777767</td>\n    </tr>\n    <tr>\n      <th>6606</th>\n      <td>BP_TargetAI_C_0</td>\n      <td>0.559256</td>\n      <td>0.461129</td>\n      <td>-1667.363647</td>\n      <td>-604.030945</td>\n      <td>True</td>\n      <td>1682525777804</td>\n    </tr>\n  </tbody>\n</table>\n<p>852 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_df[cam_df['Ped_ID'] == 'BP_TargetAI_C_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-axis  is the 10 different short tracklet during the time window, the y-axis is the local IMU data. The color represents the probability to assign. To assign the IMU to visual tracklet, we expect grids on the diagonal to be darker."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![title](image/rl.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![title](image/model.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = np.loadtxt('RL/output/result.txt', dtype=int)\n",
    "draw_grid_map(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "query should be unbatched 2D or batched 3D tensor but received 4-D query tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m src \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m512\u001B[39m))\n\u001B[0;32m      4\u001B[0m tgt \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m512\u001B[39m))\n\u001B[1;32m----> 5\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:146\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m src\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_model \u001B[38;5;129;01mor\u001B[39;00m tgt\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_model:\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe feature number of src and tgt must be equal to d_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 146\u001B[0m memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(tgt, memory, tgt_mask\u001B[38;5;241m=\u001B[39mtgt_mask, memory_mask\u001B[38;5;241m=\u001B[39mmemory_mask,\n\u001B[0;32m    148\u001B[0m                       tgt_key_padding_mask\u001B[38;5;241m=\u001B[39mtgt_key_padding_mask,\n\u001B[0;32m    149\u001B[0m                       memory_key_padding_mask\u001B[38;5;241m=\u001B[39mmemory_key_padding_mask)\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:280\u001B[0m, in \u001B[0;36mTransformerEncoder.forward\u001B[1;34m(self, src, mask, src_key_padding_mask)\u001B[0m\n\u001B[0;32m    277\u001B[0m         src_key_padding_mask_for_layers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 280\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msrc_key_padding_mask_for_layers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_nested:\n\u001B[0;32m    283\u001B[0m     output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_padded_tensor(\u001B[38;5;241m0.\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:538\u001B[0m, in \u001B[0;36mTransformerEncoderLayer.forward\u001B[1;34m(self, src, src_mask, src_key_padding_mask)\u001B[0m\n\u001B[0;32m    536\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x))\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 538\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sa_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    539\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(x))\n\u001B[0;32m    541\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:546\u001B[0m, in \u001B[0;36mTransformerEncoderLayer._sa_block\u001B[1;34m(self, x, attn_mask, key_padding_mask)\u001B[0m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sa_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor,\n\u001B[0;32m    545\u001B[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 546\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    548\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(x)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1167\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001B[0m\n\u001B[0;32m   1156\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[0;32m   1157\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[0;32m   1158\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1164\u001B[0m         q_proj_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_proj_weight, k_proj_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_proj_weight,\n\u001B[0;32m   1165\u001B[0m         v_proj_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv_proj_weight, average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights)\n\u001B[0;32m   1166\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1167\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1169\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1170\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1171\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[0;32m   1176\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\functional.py:5005\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001B[0m\n\u001B[0;32m   4975\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tens_ops):\n\u001B[0;32m   4976\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m   4977\u001B[0m         multi_head_attention_forward,\n\u001B[0;32m   4978\u001B[0m         tens_ops,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5002\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[0;32m   5003\u001B[0m     )\n\u001B[1;32m-> 5005\u001B[0m is_batched \u001B[38;5;241m=\u001B[39m \u001B[43m_mha_shape_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5007\u001B[0m \u001B[38;5;66;03m# For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\u001B[39;00m\n\u001B[0;32m   5008\u001B[0m \u001B[38;5;66;03m# is batched, run the computation and before returning squeeze the\u001B[39;00m\n\u001B[0;32m   5009\u001B[0m \u001B[38;5;66;03m# batch dimension so that the output doesn't carry this temporary batch dimension.\u001B[39;00m\n\u001B[0;32m   5010\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched:\n\u001B[0;32m   5011\u001B[0m     \u001B[38;5;66;03m# unsqueeze if the input is unbatched\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\functional.py:4881\u001B[0m, in \u001B[0;36m_mha_shape_check\u001B[1;34m(query, key, value, key_padding_mask, attn_mask, num_heads)\u001B[0m\n\u001B[0;32m   4878\u001B[0m             \u001B[38;5;28;01massert\u001B[39;00m attn_mask\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m expected_shape, \\\n\u001B[0;32m   4879\u001B[0m                 (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected `attn_mask` shape to be \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexpected_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattn_mask\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   4880\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 4881\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m   4882\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery should be unbatched 2D or batched 3D tensor but received \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-D query tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   4884\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m is_batched\n",
      "\u001B[1;31mAssertionError\u001B[0m: query should be unbatched 2D or batched 3D tensor but received 4-D query tensor"
     ]
    }
   ],
   "source": [
    "from torch.nn import Transformer\n",
    "transformer_model = Transformer(nhead=16, num_encoder_layers=12)\n",
    "# src = torch.rand((1, 50, 10, 64))\n",
    "# tgt = torch.rand((1, 50, 10, 64))\n",
    "# out = transformer_model(src, tgt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([20, 32, 512])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\liuxi/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "C:\\Users\\liuxi\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:35: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (50, 20)\u001B[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\liuxi\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:35: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (50, 6)\u001B[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\liuxi\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:22: UserWarning: \u001B[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: float32. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001B[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\liuxi\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:27: UserWarning: \u001B[33mWARN: It seems a Box observation space is an image but the lower and upper bounds are not [0, 255]. Actual lower bound: -1.0, upper bound: 1.0. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import RL\n",
    "\n",
    "env = gym.make('RL/RLfuse-v0')\n",
    "env.load_data(wifi_df, cam_df, imu_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from RL.algorithm.rlfuse_ppo import Rlfuse_ppo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning at timestep  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuxi\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:181: UserWarning: \u001B[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001B[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mRL\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malgorithm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrlfuse_ppo\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Rlfuse_ppo\n\u001B[0;32m      2\u001B[0m agent \u001B[38;5;241m=\u001B[39m Rlfuse_ppo(env, fill_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m, lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.005\u001B[39m, load_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m100000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Git Repo\\WirelessDT\\Source\\WiTracingPy\\RL\\algorithm\\rlfuse_ppo.py:210\u001B[0m, in \u001B[0;36mRlfuse_ppo.learn\u001B[1;34m(self, total_timesteps)\u001B[0m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLearning at timestep \u001B[39m\u001B[38;5;124m\"\u001B[39m, t_so_far)\n\u001B[0;32m    209\u001B[0m \u001B[38;5;66;03m# ALG STEP 3\u001B[39;00m\n\u001B[1;32m--> 210\u001B[0m batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, batch_msp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;66;03m# save the batch average matching success rate (percentage)\u001B[39;00m\n\u001B[0;32m    213\u001B[0m batch_msp_mean \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(batch_msp) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_msp)\n",
      "File \u001B[1;32m~\\Git Repo\\WirelessDT\\Source\\WiTracingPy\\RL\\algorithm\\rlfuse_ppo.py:150\u001B[0m, in \u001B[0;36mRlfuse_ppo.rollout\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    147\u001B[0m batch_obs_wifi_rssi\u001B[38;5;241m.\u001B[39mappend(wifi_rssi_lists)\n\u001B[0;32m    149\u001B[0m obs \u001B[38;5;241m=\u001B[39m (wifi_name_lists, wifi_rssi_lists, imu_list, vis_list)\n\u001B[1;32m--> 150\u001B[0m action, log_prob \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;66;03m# obs, rew, done, _ = self.env.step(action)\u001B[39;00m\n\u001B[0;32m    152\u001B[0m obs, rew, terminated, truncated, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\Git Repo\\WirelessDT\\Source\\WiTracingPy\\RL\\algorithm\\rlfuse_ppo.py:62\u001B[0m, in \u001B[0;36mRlfuse_ppo.get_action\u001B[1;34m(self, obs)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_action\u001B[39m(\u001B[38;5;28mself\u001B[39m, obs):\n\u001B[1;32m---> 62\u001B[0m     mean, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;66;03m# Create our Multivariate Normal Distribution\u001B[39;00m\n\u001B[0;32m     64\u001B[0m     dist \u001B[38;5;241m=\u001B[39m MultivariateNormal(mean, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcov_mat)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Git Repo\\WirelessDT\\Source\\WiTracingPy\\RL\\agent\\fusion_model_v2.py:92\u001B[0m, in \u001B[0;36mFusionModelv2.forward\u001B[1;34m(self, obs)\u001B[0m\n\u001B[0;32m     89\u001B[0m combined_features \u001B[38;5;241m=\u001B[39m combined_features\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, wifi_features\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m imu_features\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m visual_features\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# Compute similarity scores\u001B[39;00m\n\u001B[1;32m---> 92\u001B[0m similarity_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241m.\u001B[39mmatching_layer(combined_features)\n\u001B[0;32m     94\u001B[0m \u001B[38;5;66;03m# Reshape similarity_scores back to (batch_size, time_steps, num_tracklets)\u001B[39;00m\n\u001B[0;32m     95\u001B[0m similarity_scores \u001B[38;5;241m=\u001B[39m similarity_scores\u001B[38;5;241m.\u001B[39mview(batch_size, time_steps, num_tracklets)\n",
      "File \u001B[1;32m~\\Git Repo\\WirelessDT\\Source\\WiTracingPy\\RL\\agent\\fusion_model_v2.py:92\u001B[0m, in \u001B[0;36mFusionModelv2.forward\u001B[1;34m(self, obs)\u001B[0m\n\u001B[0;32m     89\u001B[0m combined_features \u001B[38;5;241m=\u001B[39m combined_features\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, wifi_features\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m imu_features\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m visual_features\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# Compute similarity scores\u001B[39;00m\n\u001B[1;32m---> 92\u001B[0m similarity_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241m.\u001B[39mmatching_layer(combined_features)\n\u001B[0;32m     94\u001B[0m \u001B[38;5;66;03m# Reshape similarity_scores back to (batch_size, time_steps, num_tracklets)\u001B[39;00m\n\u001B[0;32m     95\u001B[0m similarity_scores \u001B[38;5;241m=\u001B[39m similarity_scores\u001B[38;5;241m.\u001B[39mview(batch_size, time_steps, num_tracklets)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from RL.algorithm.rlfuse_ppo import Rlfuse_ppo\n",
    "agent = Rlfuse_ppo(env, fill_value=0.05, lr=0.005, load_weight=True)\n",
    "agent.learn(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "TO-DO:\n",
    "1. add more AI to the scene, display performance (e.g. matching success rate)\n",
    "2. add label order shuffle;\n",
    "3. add validation set\n",
    "4. add noise to data\n",
    "5. add time step interval (e.g. 5 time step between each selection?)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "agent_eval = Rlfuse_ppo(env, load_weight=True)\n",
    "obs, info = env.get_evaluate_obs(170)\n",
    "action = agent_eval.evaluate_action(obs)\n",
    "action_idx, reward = env.evaluate_action(action)\n",
    "imu_vis = obs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "35"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(reward)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "highest_probabilities = np.max(action, axis=-1).squeeze()\n",
    "max_indices = np.squeeze(np.argmax(action, axis=-1))\n",
    "\n",
    "# Replace indices with -1 where the highest probability is lower than 0.5\n",
    "modified_indices = np.where(highest_probabilities >= 0.7, max_indices, -1)\n",
    "\n",
    "# Convert the modified_indices numpy array to a list\n",
    "modified_indices_list = modified_indices.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from RL.agent.fusion_model_v2 import FusionModelv2\n",
    "\n",
    "model = FusionModelv2(imu_feature_size=32, visual_feature_size=32, lstm_hidden_size=64, max_pedestrian_detections=10, num_classes=10)\n",
    "\n",
    "input_list = np.random.rand(10, 50, 10, 2)\n",
    "imu_list = np.random.rand(10, 50, 6)\n",
    "input = torch.from_numpy(input_list).float()\n",
    "input_imu = torch.from_numpy(imu_list).float()\n",
    "input = (input_imu, input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\WirelessDT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Git Repo\\WirelessDT\\Source\\WiTracingPy\\RL\\agent\\fusion_model_v2.py:75\u001B[0m, in \u001B[0;36mFusionModelv2.forward\u001B[1;34m(self, obs)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;66;03m# Compute matching probabilities using softmax\u001B[39;00m\n\u001B[0;32m     73\u001B[0m matching_probs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(similarity_scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmatching_probs\u001B[49m\n",
      "File \u001B[1;32m~\\Git Repo\\WirelessDT\\Source\\WiTracingPy\\RL\\agent\\fusion_model_v2.py:75\u001B[0m, in \u001B[0;36mFusionModelv2.forward\u001B[1;34m(self, obs)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;66;03m# Compute matching probabilities using softmax\u001B[39;00m\n\u001B[0;32m     73\u001B[0m matching_probs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(similarity_scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmatching_probs\u001B[49m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model(input)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
